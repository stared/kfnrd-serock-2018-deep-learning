{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "3 May 2018: [Obóz wielodyscyplinarny Krajowego Funduszu na rzecz Dzieci](http://fundusz.org/2018/04/wkrotce-oboz-w-serocku/) by [Piotr Migdał](http://p.migdal.pl/)\n",
    "\n",
    "\n",
    "For further materials, see:\n",
    "\n",
    "* [Learning Deep Learning with Keras](http://p.migdal.pl/2017/04/30/teaching-deep-learning.html)\n",
    "* [Data science intro for math/phys background](http://p.migdal.pl/2016/03/15/data-science-intro-for-math-phys-background.html)\n",
    "* [Starting deep learning hands-on: image classification on CIFAR-10](https://blog.deepsense.ai/deep-learning-hands-on-image-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter recognition\n",
    "\n",
    "> Indeed, I once even proposed that the toughest challenge facing AI workers is to answer the question: “What are the letters ‘A’ and ‘I’? - [Douglas R. Hofstadter](https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html) (1995)\n",
    "\n",
    "\n",
    "## notMNIST\n",
    "\n",
    "\n",
    "Data source: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) (you need to download `notMNIST_small.mat` file):\n",
    "\n",
    "![](http://yaroslavvb.com/upload/notMNIST/nmn.png)\n",
    "\n",
    "> some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n",
    "\n",
    "> Approaching 0.5% error rate on notMNIST_small would be very impressive. If you run your algorithm on this dataset, please let me know your results.\n",
    "\n",
    "\n",
    "## So, why not MNIST?\n",
    "\n",
    "Many introductions to image classification with deep learning start with MNIST, a standard dataset of handwritten digits. This is unfortunate. Not only does it not produce a “Wow!” effect or show where deep learning shines, but it also can be solved with shallow machine learning techniques. In this case, plain k-Nearest Neighbors produces more than 97% accuracy (or even 99.5% with some data preprocessing!). Moreover, MNIST is not a typical image dataset – and mastering it is unlikely to teach you transferable skills that would be useful for other classification problems\n",
    "\n",
    "> Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and no[t] transfer to real [computer vision]. - [François Chollet’s tweet](https://twitter.com/fchollet/status/852594987527045120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Local\n",
    "\n",
    "* Python 3.6 with Anaconda\n",
    "* Keras 2.1.4\n",
    "* TensorFlow (for Keras backend)\n",
    "\n",
    "Installing additional packages - [keras-sequential-ascii](https://github.com/stared/keras-sequential-ascii) and [livelossplot](https://github.com/stared/livelossplot).\n",
    "\n",
    "We will use Keras 2.1.4. With [2.1.6 there might be some problems](https://github.com/keras-team/keras/issues/9621). Use `pip install -U keras==2.1.4` if needed.\n",
    "\n",
    "### Neptune\n",
    "\n",
    "If using on [Neptune - Machine Learning Lab](https://neptune.ml/) - create an account there. Then, create a new notebook:\n",
    "\n",
    "* medium cpu is enough\n",
    "* python 3\n",
    "* keras 2.1.4\n",
    "* make sure to upload this file!\n",
    "\n",
    "![](img/neptune_notebook.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install livelossplot\n",
    "!pip install keras-sequential-ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading data (112 MB). If needed, I have it on my pendrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# data preprocessing\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization, GlobalMaxPool2D\n",
    "\n",
    "# keras vis\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras_sequential_ascii import keras2ascii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = io.loadmat(\"notMNIST_small.mat\")\n",
    "\n",
    "# transform data\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "resolution = 28\n",
    "classes = 10\n",
    "\n",
    "X = np.transpose(X, (2, 0, 1))\n",
    "\n",
    "y = y.astype('int32')\n",
    "X = X.astype('float32') / 255.\n",
    "\n",
    "# shape: (sample, x, y, channel)\n",
    "X = X.reshape((-1, resolution, resolution, 1))\n",
    "\n",
    "# 3 -> [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]\n",
    "Y = np_utils.to_categorical(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at data; some fonts are strange\n",
    "i = 42\n",
    "plt.imshow(X[i,:,:,0])\n",
    "plt.title(\"ABCDEFGHIJ\"[y[i]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random letters\n",
    "rows = 6\n",
    "fig, axs = plt.subplots(rows, classes, figsize=(classes, rows))\n",
    "for letter_id in range(10):\n",
    "    letters = X[y == letter_id]\n",
    "    for i in range(rows):\n",
    "        ax = axs[i, letter_id]\n",
    "        ax.imshow(letters[np.random.randint(len(letters)),:,:,0],\n",
    "                  cmap='Greys', interpolation='none')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "A simple, shallow method. Uses [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "\n",
    "It works as follows:\n",
    "\n",
    "* flattens input to a single vector\n",
    "* multiplies by a matrix\n",
    "* applies softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(resolution, resolution, 1)))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, X=X_test, Y=Y_test, rows=8):\n",
    "    # example predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    rows = 8\n",
    "    fig, axs = plt.subplots(rows, 2, figsize=(8, 1.5 * rows))\n",
    "    for i in range(rows):\n",
    "        ax = axs[i,0]\n",
    "        idx = np.random.randint(len(X_test))\n",
    "        ax.imshow(X_test[idx,:,:,0], cmap='Greys', interpolation='none')\n",
    "        ax.axis('off')\n",
    "\n",
    "        pd.Series(Y_test[idx], index=list(\"ABCDEFGHIJ\")).plot('bar', ax=axs[i,1], ylim=[0,1], color=plt.cm.Set1(0))\n",
    "        pd.Series(predictions[idx], index=list(\"ABCDEFGHIJ\")).plot('bar', ax=axs[i,1], ylim=[0,1], color=plt.cm.Set1(1))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron (MLP)\n",
    "\n",
    "An old-school network - only dense layers, sigmoid (or tanh) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(resolution, resolution, 1)))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "See [Image Kernels - Visually Explained](http://setosa.io/ev/image-kernels/).\n",
    "\n",
    "Change optimizer from `sgd` to `adam`; see:\n",
    "\n",
    "* [Why Momentum Really Works](https://distill.pub/2017/momentum/)\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)\n",
    "* [SGD > Adam?? Which One Is The Best Optimizer: Dogs-VS-Cats Toy Experiment](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution  + MaxPool\n",
    "\n",
    "More on typical blocks in [Convolutional Neural Networks (CNNs / ConvNets)](http://cs231n.github.io/convolutional-networks/) by Andrej Karpathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical ConvNet architecture\n",
    "\n",
    "It uses hierarchical features. It allows to\n",
    "\n",
    "* [How convolutional neural networks see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) - Keras blog\n",
    "* [How neural networks build up their understanding of images](https://distill.pub/2017/feature-visualization/) - distill.pub\n",
    "* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) - distill.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More dense layers, dropout\n",
    "\n",
    "Usually we use 2-3 dense layers. To prevent overfitting we use **dropout**.\n",
    "\n",
    "\n",
    "* Hinton et al, [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580), 2012\n",
    "\n",
    "![](img/dropout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Often we can speed-up training by using batch normalization. It is especially useful for deep neural networks.\n",
    "\n",
    "* [Understanding the backward pass through Batch Normalization Layer](http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
    "* [On The Perils of Batch Norm](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fully convolutional neural networks\n",
    "\n",
    "Sometimes we want a network, which is fully translationally-invariant and can accept images of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(resolution, resolution, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(GlobalMaxPool2D())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2ascii(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
